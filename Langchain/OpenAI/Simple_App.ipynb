{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Gen AI APP Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1c8bb9359c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nLog traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonSearchRegionUSEUGo to AppQuick StartObservabilityTutorialsAdd observability to your LLM applicationHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsMonitoring and automationsConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesTracingLog traces to specific projectOn this pageLog traces to specific project\\nYou can change the destination project of your traces both statically through environment variables and dynamically at runtime.\\nSet the destination project statically‚Äã\\nAs mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application.\\nexport LANGCHAIN_PROJECT=my-custom-project\\nIf the project specified does not exist, it will be created automatically when the first trace is ingested.\\nSet the destination project dynamically‚Äã\\nYou can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application.\\nnoteSetting the project name dynamically using one of the below methods overrides the project name set by the LANGCHAIN_PROJECT environment variable.\\nPythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages = [  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  {\"role\": \"user\", \"content\": \"Hello!\"}]# Use the @traceable decorator with the \\'project_name\\' parameter to log traces to LangSmith# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for @traceable to work@traceable(  run_type=\"llm\",  name=\"OpenAI Call Decorator\",  project_name=\"My Project\")def call_openai(  messages: list[dict], model: str = \"gpt-4o-mini\") -> str:  return client.chat.completions.create(      model=model,      messages=messages,  ).choices[0].message.content# Call the decorated functioncall_openai(messages)# You can also specify the Project via the project_name parameter# This will override the project_name specified in the @traceable decoratorcall_openai(  messages,  langsmith_extra={\"project_name\": \"My Overridden Project\"},)# The wrapped OpenAI client accepts all the same langsmith_extra parameters# as @traceable decorated functions, and logs traces to LangSmith automatically.# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for the wrapper to work.from langsmith import wrapperswrapped_client = wrappers.wrap_openai(client)wrapped_client.chat.completions.create(  model=\"gpt-4o-mini\",  messages=messages,  langsmith_extra={\"project_name\": \"My Project\"},)# Alternatively, create a RunTree object# You can set the project name using the project_name parameterrt = RunTree(  run_type=\"llm\",  name=\"OpenAI Call RunTree\",  inputs={\"messages\": messages},  project_name=\"My Project\")chat_completion = client.chat.completions.create(  model=\"gpt-4o-mini\",  messages=messages,)# End and submit the runrt.end(outputs=chat_completion)rt.post()import OpenAI from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";import { RunTree} from \"langsmith\";const client = new OpenAI();const messages = [  {role: \"system\", content: \"You are a helpful assistant.\"},  {role: \"user\", content: \"Hello!\"}];const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[], model: string) => {  const completion = await client.chat.completions.create({      model: model,      messages: messages,  });  return completion.choices[0].message.content;},{  run_type: \"llm\",  name: \"OpenAI Call Traceable\",  project_name: \"My Project\"});// Call the traceable functionawait traceableCallOpenAI(messages, \"gpt-4o-mini\");// Create and use a RunTree objectconst rt = new RunTree({  runType: \"llm\",  name: \"OpenAI Call RunTree\",  inputs: { messages },  project_name: \"My Project\"});await rt.postRun();// Execute a chat completion and handle it within RunTreert.end({outputs: chatCompletion});await rt.patchRun();Was this page helpful?You can leave detailed feedback on GitHub.PreviousUpload files with tracesNextSet a sampling rate for tracesSet the destination project staticallySet the destination project dynamicallyCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data--> Docs-->Divide our Docuemnts into chunks dcouments-->text-->vectors-->Vector Embeddings--->Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='Skip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonSearchRegionUSEUGo to AppQuick StartObservabilityTutorialsAdd observability to your LLM applicationHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsMonitoring and automationsConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesTracingLog traces to specific projectOn this pageLog traces to specific project'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='You can change the destination project of your traces both statically through environment variables and dynamically at runtime.\\nSet the destination project statically‚Äã\\nAs mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application.\\nexport LANGCHAIN_PROJECT=my-custom-project\\nIf the project specified does not exist, it will be created automatically when the first trace is ingested.\\nSet the destination project dynamically‚Äã\\nYou can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='noteSetting the project name dynamically using one of the below methods overrides the project name set by the LANGCHAIN_PROJECT environment variable.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='PythonTypeScriptimport openaifrom langsmith import traceablefrom langsmith.run_trees import RunTreeclient = openai.Client()messages = [  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  {\"role\": \"user\", \"content\": \"Hello!\"}]# Use the @traceable decorator with the \\'project_name\\' parameter to log traces to LangSmith# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for @traceable to work@traceable(  run_type=\"llm\",  name=\"OpenAI Call Decorator\",  project_name=\"My Project\")def call_openai(  messages: list[dict], model: str = \"gpt-4o-mini\") -> str:  return client.chat.completions.create(      model=model,      messages=messages,  ).choices[0].message.content# Call the decorated functioncall_openai(messages)# You can also specify the Project via the project_name parameter# This will override the project_name specified in the @traceable decoratorcall_openai(  messages,  langsmith_extra={\"project_name\": \"My Overridden Project\"},)# The wrapped OpenAI client'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='parameter# This will override the project_name specified in the @traceable decoratorcall_openai(  messages,  langsmith_extra={\"project_name\": \"My Overridden Project\"},)# The wrapped OpenAI client accepts all the same langsmith_extra parameters# as @traceable decorated functions, and logs traces to LangSmith automatically.# Ensure that the LANGCHAIN_TRACING_V2 environment variables is set for the wrapper to work.from langsmith import wrapperswrapped_client = wrappers.wrap_openai(client)wrapped_client.chat.completions.create(  model=\"gpt-4o-mini\",  messages=messages,  langsmith_extra={\"project_name\": \"My Project\"},)# Alternatively, create a RunTree object# You can set the project name using the project_name parameterrt = RunTree(  run_type=\"llm\",  name=\"OpenAI Call RunTree\",  inputs={\"messages\": messages},  project_name=\"My Project\")chat_completion = client.chat.completions.create(  model=\"gpt-4o-mini\",  messages=messages,)# End and submit the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='name=\"OpenAI Call RunTree\",  inputs={\"messages\": messages},  project_name=\"My Project\")chat_completion = client.chat.completions.create(  model=\"gpt-4o-mini\",  messages=messages,)# End and submit the runrt.end(outputs=chat_completion)rt.post()import OpenAI from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";import { RunTree} from \"langsmith\";const client = new OpenAI();const messages = [  {role: \"system\", content: \"You are a helpful assistant.\"},  {role: \"user\", content: \"Hello!\"}];const traceableCallOpenAI = traceable(async (messages: {role: string, content: string}[], model: string) => {  const completion = await client.chat.completions.create({      model: model,      messages: messages,  });  return completion.choices[0].message.content;},{  run_type: \"llm\",  name: \"OpenAI Call Traceable\",  project_name: \"My Project\"});// Call the traceable functionawait traceableCallOpenAI(messages, \"gpt-4o-mini\");// Create and use a'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='run_type: \"llm\",  name: \"OpenAI Call Traceable\",  project_name: \"My Project\"});// Call the traceable functionawait traceableCallOpenAI(messages, \"gpt-4o-mini\");// Create and use a RunTree objectconst rt = new RunTree({  runType: \"llm\",  name: \"OpenAI Call RunTree\",  inputs: { messages },  project_name: \"My Project\"});await rt.postRun();// Execute a chat completion and handle it within RunTreert.end({outputs: chatCompletion});await rt.patchRun();Was this page helpful?You can leave detailed feedback on GitHub.PreviousUpload files with tracesNextSet a sampling rate for tracesSet the destination project staticallySet the destination project dynamicallyCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1c8e7921cf0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.\\nSet the destination project statically‚Äã\\nAs mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application.\\nexport LANGCHAIN_PROJECT=my-custom-project\\nIf the project specified does not exist, it will be created automatically when the first trace is ingested.\\nSet the destination project dynamically‚Äã\\nYou can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Query From a vectordb\n",
    "query=\"How to set the destination\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001C8E7922EC0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001C8E727B130>, root_client=<openai.OpenAI object at 0x000001C8BB936B60>, root_async_client=<openai.AsyncOpenAI object at 0x000001C8E7923B20>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To change the destination project of your traces statically, you need to set the `LANGCHAIN_PROJECT` environment variable to your desired project name before executing your application. For example, you can run the command `export LANGCHAIN_PROJECT=my-custom-project`. If the specified project does not exist, it will be created automatically when the first trace is ingested.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"How to set the destination\",\n",
    "    \"context\":[Document(page_content=\"You can change the destination project of your traces both statically through environment variables and dynamically at runtime.\\nSet the destination project statically‚Äã\\nAs mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application.\\nexport LANGCHAIN_PROJECT=my-custom-project\\nIf the project specified does not exist, it will be created automatically when the first trace is ingested. \")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1c8e7921cf0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input--->Retriever--->vectorstoredb\n",
    "\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C8E7921CF0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001C8E7922EC0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001C8E727B130>, root_client=<openai.OpenAI object at 0x000001C8BB936B60>, root_async_client=<openai.AsyncOpenAI object at 0x000001C8E7923B20>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To set the destination project for your traces dynamically at runtime, you can use various methods depending on how you are annotating your code for tracing. This allows you to log traces to different projects within the same application. When you set the project name dynamically, it overrides any project name set by the LANGCHAIN_PROJECT environment variable.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get the response form the LLM\n",
    "response=retrieval_chain.invoke({\"input\":\"How to set the destination\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How to set the destination',\n",
       " 'context': [Document(id='aa9d5352-e117-4506-916f-ba1c186a398b', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='You can change the destination project of your traces both statically through environment variables and dynamically at runtime.\\nSet the destination project statically‚Äã\\nAs mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application.\\nexport LANGCHAIN_PROJECT=my-custom-project\\nIf the project specified does not exist, it will be created automatically when the first trace is ingested.\\nSet the destination project dynamically‚Äã\\nYou can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application.'),\n",
       "  Document(id='f2ac3917-4e0b-4b08-af03-f48418eb4b15', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'),\n",
       "  Document(id='faf88f70-007d-405b-9abd-09c975c000e2', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsMonitoring and automationsConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesTracingLog traces to specific projectOn this pageLog traces to specific project'),\n",
       "  Document(id='23209bc7-59da-498e-91b4-d15b4a247255', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='noteSetting the project name dynamically using one of the below methods overrides the project name set by the LANGCHAIN_PROJECT environment variable.')],\n",
       " 'answer': 'To set the destination project for your traces dynamically at runtime, you can use various methods depending on how you are annotating your code for tracing. This allows you to log traces to different projects within the same application. When you set the project name dynamically, it overrides any project name set by the LANGCHAIN_PROJECT environment variable.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='aa9d5352-e117-4506-916f-ba1c186a398b', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='You can change the destination project of your traces both statically through environment variables and dynamically at runtime.\\nSet the destination project statically‚Äã\\nAs mentioned in the Tracing Concepts section, LangSmith uses the concept of a Project to group traces. If left unspecified, the project is set to default. You can set the LANGCHAIN_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application.\\nexport LANGCHAIN_PROJECT=my-custom-project\\nIf the project specified does not exist, it will be created automatically when the first trace is ingested.\\nSet the destination project dynamically‚Äã\\nYou can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application.'),\n",
       " Document(id='f2ac3917-4e0b-4b08-af03-f48418eb4b15', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith'),\n",
       " Document(id='faf88f70-007d-405b-9abd-09c975c000e2', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsMonitoring and automationsConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityHow-to GuidesTracingLog traces to specific projectOn this pageLog traces to specific project'),\n",
       " Document(id='23209bc7-59da-498e-91b4-d15b4a247255', metadata={'source': 'https://docs.smith.langchain.com/observability/how_to_guides/tracing/log_traces_to_project', 'title': 'Log traces to specific project | \\uf8ffü¶úÔ∏è\\uf8ffüõ†Ô∏è LangSmith', 'description': 'You can change the destination project of your traces both statically through environment variables and dynamically at runtime.', 'language': 'en'}, page_content='noteSetting the project name dynamically using one of the below methods overrides the project name set by the LANGCHAIN_PROJECT environment variable.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
